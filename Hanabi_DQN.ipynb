{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9hrYDHlWyuL"
      },
      "source": [
        "# Hanabi + dqn\n",
        "\n",
        "see also tutorial https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
      ]
    },
    
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkBQUSatWx61",
        "outputId": "094a28c2-4306-4061-e99f-9f309ca9eb06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/google-deepmind/hanabi-learning-environment.git\n",
            "  Cloning https://github.com/google-deepmind/hanabi-learning-environment.git to /tmp/pip-req-build-3d7hi4mo\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google-deepmind/hanabi-learning-environment.git /tmp/pip-req-build-3d7hi4mo\n",
            "  Resolved https://github.com/google-deepmind/hanabi-learning-environment.git to commit 54e79594f4b6fb40ebb3004289c6db0e34a8b5fb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from hanabi_learning_environment==0.0.1) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->hanabi_learning_environment==0.0.1) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/google-deepmind/hanabi-learning-environment.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLsy4DDVXiIv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from hanabi_learning_environment import rl_env\n",
        "from hanabi_learning_environment.agents.random_agent import RandomAgent\n",
        "from hanabi_learning_environment.agents.simple_agent import SimpleAgent\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "FORMAT = '%(asctime)s %(levelname)s %(message)s'\n",
        "logging.basicConfig(format=FORMAT, level=logging.INFO, force=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8HUEqIrXtKI"
      },
      "source": [
        "## env with wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhj06UFOml73"
      },
      "outputs": [],
      "source": [
        "def card2s(card):\n",
        "  color = card['color']\n",
        "  rank = card['rank']\n",
        "  return f'{color or \"?\"}' + f'{\"_\" if rank is None or rank < 0 else int(rank)}'\n",
        "def hand2s(hand):\n",
        "  return ','.join([card2s(_) for _ in hand])\n",
        "def hanabi2s(fireworks):\n",
        "  ret = []\n",
        "  for key in sorted(fireworks.keys()):\n",
        "    ret += [f'{key}{fireworks[key]}']\n",
        "  return ','.join(ret)\n",
        "def index_to_binaryvector(indices, length):\n",
        "  \"\"\"a tool to mask illegal moves\"\"\"\n",
        "  a = torch.zeros(length)\n",
        "  a.scatter_(0, torch.Tensor(indices).to(torch.int64), 1)\n",
        "  return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlUdTMUDXsL3"
      },
      "outputs": [],
      "source": [
        "class GameRecord:\n",
        "  def __init__(self):\n",
        "    self.player_obs = []\n",
        "    self.actions = []\n",
        "    self.action_names = []\n",
        "    self.rewards = []\n",
        "    #self.intrinsic_rewards=[]\n",
        "    self.cooperation_rate=[]\n",
        "    self.play_discard_rate=[]\n",
        "    self.hanabi_scores=[]\n",
        "    self.history_for_human = []\n",
        "  def episode_return(self):\n",
        "    return sum(self.rewards)\n",
        "  def __len__(self):\n",
        "    return len(self.actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_z6ImzLX8wy"
      },
      "outputs": [],
      "source": [
        "class HanabiEnv:\n",
        "  \"\"\"a wrapper for HLE env\"\"\"\n",
        "  def __init__(self, name='Hanabi-Full', num_players=2):\n",
        "    self.env = rl_env.make(environment_name=name, num_players=num_players)\n",
        "    # default make(environment_name='Hanabi-Full', num_players=2, pyhanabi_path=None)\n",
        "    # env = rl_env.make(environment_name='Hanabi-Small')\n",
        "    self.N = num_players\n",
        "    self.reset()\n",
        "  def reset(self):\n",
        "    self.observations = self.env.reset()\n",
        "    self.record = GameRecord()\n",
        "    self.reward = 0\n",
        "    self.intrinsic_reward=0\n",
        "    self.done = False\n",
        "    self.update()\n",
        "  def step(self, action: int):\n",
        "    if action not in self.legal_moves():\n",
        "      raise RuntimeError(f'action {action} not in {self.legal_moves}')\n",
        "\n",
        "    self.observations, self.reward, self.done, _ = self.env.step(action)\n",
        "    self.update(action)\n",
        "  def obs(self, agent_id: int | None = None):\n",
        "    if agent_id is None:\n",
        "      agent_id = self.to_play\n",
        "    return self.local_obs[agent_id]\n",
        "  def legal_moves(self, agent_id: int | None = None):\n",
        "    return self.obs(agent_id)['legal_moves_as_int']\n",
        "  def legal_moves_dict(self, agent_id: int | None = None):\n",
        "    return self.obs(agent_id)['legal_moves']\n",
        "  def vector_obs(self, agent_id: int | None = None):\n",
        "    return self.obs()['vectorized']\n",
        "  def update(self, action: int | None = None):\n",
        "    if action is not None:\n",
        "      self.record.actions.append(action)\n",
        "      action_ord = self.legal_moves().index(action)\n",
        "      name = list(self.obs()['legal_moves'][action_ord].values())\n",
        "      self.record.action_names.append(str(name))\n",
        "      self.record.rewards.append(self.reward)\n",
        "      #append intrinsic reward\n",
        "      #self.record.intrinsic_rewards.append(self.intrinsic_reward)\n",
        "    self.to_play = self.observations['current_player']\n",
        "    self.local_obs = [self.observations['player_observations'][_]\n",
        "                      for _ in range(self.N)]\n",
        "    self.record.player_obs.append(self.obs()['vectorized'])\n",
        "    self.record.history_for_human.append(self.debug_info())\n",
        "  def debug_info(self):\n",
        "    obs = self.obs()\n",
        "    s = ''\n",
        "    s += f'Fireworks: {hanabi2s(obs[\"fireworks\"])} life: {obs[\"life_tokens\"]}' \\\n",
        "         + f' info: {obs[\"information_tokens\"]}'\n",
        "    for i in range(self.N):\n",
        "      s += f'  Hand{i} ' + hand2s(self.obs((i+self.N-1)%self.N)[\"observed_hands\"][1])  \\\n",
        "           + f'  Know{i} ' + hand2s(self.obs(i)[\"card_knowledge\"][0])\n",
        "    return s\n",
        "\n",
        "    def calculate_score(self,agent_id):\n",
        "        \"\"\"Calculate the final score based on the fireworks state.\"\"\"\n",
        "        return sum(self.obs(agent_id)['fireworks'].values())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k1oh_-jmeSc"
      },
      "source": [
        "### env usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFPcI-pMmtQt",
        "outputId": "bd3185a3-b173-41c3-e8c7-43cfd584ad61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "index_to_binaryvector([3,8], 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JGh3lqK6YfDW",
        "outputId": "57a3b54e-af6e-4a8f-ae6f-97bf2769c4a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Fireworks: B0,G0,R0,W0,Y0 life: 3 info: 8  Hand0 Y0,G3,W0,R2,Y4  Know0 ?_,?_,?_,?_,?_  Hand1 R0,Y3,Y0,B1,W1  Know1 ?_,?_,?_,?_,?_'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env0 = HanabiEnv()\n",
        "env0.reset()\n",
        "env0.debug_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AnrT5oSYV14",
        "outputId": "0156d84e-f80d-461f-d0fa-7a8d2b03c489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18] [{'action_type': 'PLAY', 'card_index': 0}, {'action_type': 'PLAY', 'card_index': 1}, {'action_type': 'PLAY', 'card_index': 2}, {'action_type': 'PLAY', 'card_index': 3}, {'action_type': 'PLAY', 'card_index': 4}, {'action_type': 'REVEAL_COLOR', 'target_offset': 1, 'color': 'R'}, {'action_type': 'REVEAL_COLOR', 'target_offset': 1, 'color': 'Y'}, {'action_type': 'REVEAL_COLOR', 'target_offset': 1, 'color': 'W'}, {'action_type': 'REVEAL_COLOR', 'target_offset': 1, 'color': 'B'}, {'action_type': 'REVEAL_RANK', 'target_offset': 1, 'rank': 0}, {'action_type': 'REVEAL_RANK', 'target_offset': 1, 'rank': 1}, {'action_type': 'REVEAL_RANK', 'target_offset': 1, 'rank': 3}]\n"
          ]
        }
      ],
      "source": [
        "print(env0.legal_moves(), env0.legal_moves_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtRB_kGHYa4m"
      },
      "outputs": [],
      "source": [
        "def step_and_show(env, action):\n",
        "  env.step(action)\n",
        "  print(env.debug_info())\n",
        "  print(env.legal_moves(), env.legal_moves_dict())\n",
        "  if env.done:\n",
        "    print('gameends')\n",
        "  else:\n",
        "    print(f'{env.to_play=}, {env.reward=}')\n",
        "  print(f'{env.record.episode_return()=}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6gI0FnBYcYU",
        "outputId": "3fbcb81d-0bf1-4b51-fb07-f96eda365a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fireworks: B0,G0,R0,W0,Y1 life: 3 info: 8  Hand0 G3,W0,R2,Y4,Y0  Know0 ?_,?_,?_,?_,?_  Hand1 R0,Y3,Y0,B1,W1  Know1 ?_,?_,?_,?_,?_\n",
            "[5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 17, 18, 19] [{'action_type': 'PLAY', 'card_index': 0}, {'action_type': 'PLAY', 'card_index': 1}, {'action_type': 'PLAY', 'card_index': 2}, {'action_type': 'PLAY', 'card_index': 3}, {'action_type': 'PLAY', 'card_index': 4}, {'action_type': 'REVEAL_COLOR', 'target_offset': 1, 'color': 'R'}, {'action_type': 'REVEAL_COLOR', 'target_offset': 1, 'color': 'Y'}, {'action_type': 'REVEAL_COLOR', 'target_offset': 1, 'color': 'G'}, {'action_type': 'REVEAL_COLOR', 'target_offset': 1, 'color': 'W'}, {'action_type': 'REVEAL_RANK', 'target_offset': 1, 'rank': 0}, {'action_type': 'REVEAL_RANK', 'target_offset': 1, 'rank': 2}, {'action_type': 'REVEAL_RANK', 'target_offset': 1, 'rank': 3}, {'action_type': 'REVEAL_RANK', 'target_offset': 1, 'rank': 4}]\n",
            "env.to_play=1, env.reward=1\n",
            "env.record.episode_return()=1\n"
          ]
        }
      ],
      "source": [
        "step_and_show(env0, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU-_NTyvnm6-"
      },
      "source": [
        "## Agents and playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsMEKvQFnsLA"
      },
      "outputs": [],
      "source": [
        "class FCModel(torch.nn.Module):\n",
        "  \"\"\"fully connected neural networks serving as a building block of an agent\"\"\"\n",
        "  def __init__(self, obs_dim: int, action_dim: int, device, hidden_size=256):\n",
        "    super().__init__()\n",
        "    logging.info(f'model {obs_dim=}, {action_dim=}, {device=}')\n",
        "    self.device = device\n",
        "    self.action_dim= action_dim\n",
        "    self.fc = torch.nn.Sequential(\n",
        "      torch.nn.Linear(obs_dim, hidden_size),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(hidden_size, hidden_size),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(hidden_size, action_dim),\n",
        "    )\n",
        "  def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"return Q values of all actions given an observation as obs\"\"\"\n",
        "    return self.fc(obs.to(torch.float32).to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac9MKEVXxjMv"
      },
      "source": [
        "### Intrinsic reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lH-9BiOlrXc"
      },
      "source": [
        "#### Cooperative Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0fFYLvxlxCQ"
      },
      "outputs": [],
      "source": [
        "def calculate_cooperative_agent_reward(current_observation,next_observation, action_taken):\n",
        "    \"\"\"\n",
        "    Calculate the intrinsic reward\n",
        "\n",
        "    Parameters:\n",
        "    previous_observation (dict): The observation of the game before the action was taken.\n",
        "    current_observation (dict): The observation of the game after the action was taken.\n",
        "    action_taken (dict): The action taken by the agent.\n",
        "\n",
        "    Returns:\n",
        "    float: The calculated intrinsic reward.\n",
        "    \"\"\"\n",
        "    intrinsic_reward = 0.0\n",
        "    set_reward = {'reveal_play': 2, 'repeat_reveal': -0.5}\n",
        "\n",
        "\n",
        "    cur_fireworks = current_observation['fireworks']\n",
        "\n",
        "    try:\n",
        "        action_idx = current_observation['legal_moves_as_int'].index(action_taken)\n",
        "        action_dict = current_observation['legal_moves'][action_idx]\n",
        "    except ValueError:\n",
        "        # Handle the case where action_id is not in legal_moves_as_int\n",
        "        return intrinsic_reward\n",
        "\n",
        "    action_type = action_dict['action_type']\n",
        "    cur_player_observed_hands = current_observation['observed_hands']\n",
        "    card_knowledge = current_observation['card_knowledge']\n",
        "\n",
        "    if action_type == 'PLAY':\n",
        "      intrinsic_reward-= 1\n",
        "\n",
        "\n",
        "    if action_type == 'DISCARD':\n",
        "        intrinsic_reward-= 1\n",
        "    # Reward for giving hints (cooperation)\n",
        "    if 10 <= action_taken <= 19:\n",
        "        intrinsic_reward += 1\n",
        "\n",
        "    if action_type in ['REVEAL_COLOR', 'REVEAL_RANK']:\n",
        "        target_offset = action_dict['target_offset']\n",
        "        target_hand = cur_player_observed_hands[target_offset]\n",
        "        target_knowledge = card_knowledge[target_offset]\n",
        "\n",
        "        if action_type == 'REVEAL_COLOR':\n",
        "            color = action_dict['color']\n",
        "            for card_idx, card in enumerate(target_hand):\n",
        "                if target_knowledge[card_idx]['color'] == color:\n",
        "                    intrinsic_reward += set_reward['repeat_reveal']\n",
        "                elif card['color'] == color and card['rank'] == cur_fireworks[color]:\n",
        "                    intrinsic_reward += set_reward['reveal_play']\n",
        "\n",
        "        else:  # REVEAL_RANK\n",
        "            rank = action_dict['rank']\n",
        "            for card_idx, card in enumerate(target_hand):\n",
        "                if target_knowledge[card_idx]['rank'] == rank:\n",
        "                    intrinsic_reward += set_reward['repeat_reveal']\n",
        "                elif card['rank'] == rank and cur_fireworks[card['color']] == rank:\n",
        "                    intrinsic_reward += set_reward['reveal_play']\n",
        "\n",
        "    # Penalty or reward for information token usage\n",
        "    info_token_difference = next_observation['information_tokens'] - \\\n",
        "                            current_observation['information_tokens']\n",
        "    intrinsic_reward += info_token_difference\n",
        "\n",
        "\n",
        "    return intrinsic_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdo_t-g1nhwA"
      },
      "source": [
        "#### Non Cooperative Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp6wE-NDoN7g"
      },
      "source": [
        "#####Dumb Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URQnSp2Hn65U"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Agents are rewarded for playing or discarding a card without checking if it is successful or not\n",
        "\"\"\"\n",
        "def calculate_dumb_reward(current_observation,next_observation, action_taken):\n",
        "    \"\"\"\n",
        "    Calculate the intrinsic reward\n",
        "\n",
        "    Parameters:\n",
        "    previous_observation (dict): The observation of the game before the action was taken.\n",
        "    current_observation (dict): The observation of the game after the action was taken.\n",
        "    action_taken (dict): The action taken by the agent.\n",
        "\n",
        "    Returns:\n",
        "    float: The calculated intrinsic reward.\n",
        "    \"\"\"\n",
        "    intrinsic_reward = 0.0\n",
        "\n",
        "    try:\n",
        "        action_idx = current_observation['legal_moves_as_int'].index(action_taken)\n",
        "        action_dict = current_observation['legal_moves'][action_idx]\n",
        "    except ValueError:\n",
        "        # Handle the case where action_id is not in legal_moves_as_int\n",
        "        return intrinsic_reward\n",
        "\n",
        "    action_type = action_dict['action_type']\n",
        "    cur_player_observed_hands = current_observation['observed_hands']\n",
        "    card_knowledge = current_observation['card_knowledge']\n",
        "\n",
        "\n",
        "    # Reward for certainty / progress in fireworks and successful play\n",
        "\n",
        "    if action_type == 'PLAY':\n",
        "      intrinsic_reward+= 1\n",
        "\n",
        "\n",
        "    if action_type == 'DISCARD':\n",
        "        intrinsic_reward+= 1\n",
        "\n",
        "    return intrinsic_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT7nIYZ2o3Up"
      },
      "source": [
        "##### Intuitive Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzd4wN6po_Qf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Agents are rewarded for using more intuition to play and discard cards\n",
        "\"\"\"\n",
        "def calculate_intuitive_reward(current_observation,next_observation, action_taken):\n",
        "    \"\"\"\n",
        "    Calculate the intrinsic reward\n",
        "\n",
        "    Parameters:\n",
        "    previous_observation (dict): The observation of the game before the action was taken.\n",
        "    current_observation (dict): The observation of the game after the action was taken.\n",
        "    action_taken (dict): The action taken by the agent.\n",
        "\n",
        "    Returns:\n",
        "    float: The calculated intrinsic reward.\n",
        "    \"\"\"\n",
        "    intrinsic_reward = 0.0\n",
        "    set_reward = {'reward_successful_play': 1 ,'penalty_wrong_play': -1  ,'discard_useless': 1,'discard_useful': -1}\n",
        "\n",
        "\n",
        "    cur_fireworks = current_observation['fireworks']\n",
        "\n",
        "    try:\n",
        "        action_idx = current_observation['legal_moves_as_int'].index(action_taken)\n",
        "        action_dict = current_observation['legal_moves'][action_idx]\n",
        "    except ValueError:\n",
        "        # Handle the case where action_id is not in legal_moves_as_int\n",
        "        return intrinsic_reward\n",
        "\n",
        "    action_type = action_dict['action_type']\n",
        "    cur_player_observed_hands = current_observation['observed_hands']\n",
        "    card_knowledge = current_observation['card_knowledge']\n",
        "\n",
        "    # Reward for certainty / progress in fireworks and successful play\n",
        "\n",
        "    if action_type == 'PLAY' or 'DISCARD':\n",
        "\n",
        "    # Reward for intuition:\n",
        "        intuition_score=0.1\n",
        "\n",
        "        card_index = action_dict['card_index']\n",
        "\n",
        "        if card_index >= len(card_knowledge[0]):\n",
        "            return False  # Invalid card index\n",
        "\n",
        "        card = card_knowledge[0][card_index]\n",
        "        if card['color'] is None: intuition_score += 0.45\n",
        "        if card['rank'] is None: intuition_score += 0.45\n",
        "\n",
        "        if action_type == 'PLAY':\n",
        "    #check successful or failed play\n",
        "          fireworks_progress = sum(next_observation['fireworks'].values()) - \\\n",
        "                             sum(current_observation['fireworks'].values())\n",
        "          if fireworks_progress > 0:\n",
        "            intrinsic_reward += intuition_score * set_reward['reward_successful_play']\n",
        "          else:\n",
        "            intrinsic_reward += intuition_score * set_reward['penalty_wrong_play']\n",
        "    # Reward for strategic discard\n",
        "\n",
        "        else:\n",
        "          #rank might be None so we get it from the discarded pile in next observation\n",
        "          known_discarded_card= next_observation['discard_pile'][len(next_observation['discard_pile'])-1]\n",
        "          color=known_discarded_card['color']\n",
        "          fireworks = current_observation['fireworks']\n",
        "          if known_discarded_card['rank'] == fireworks[color]+1:\n",
        "            intrinsic_reward+= intuition_score * set_reward['discard_useful']\n",
        "          else:\n",
        "            intrinsic_reward+= intuition_score * set_reward['discard_useless']\n",
        "\n",
        "    return intrinsic_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UuW4YhBy84l"
      },
      "source": [
        "#####Certain Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiVrX7cUzBP0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Agents are rewarded for playing or discarding fully known cards\n",
        "\"\"\"\n",
        "def calculate_certain_reward(current_observation,next_observation, action_taken):\n",
        "    \"\"\"\n",
        "    Calculate the intrinsic reward\n",
        "\n",
        "    Parameters:\n",
        "    previous_observation (dict): The observation of the game before the action was taken.\n",
        "    current_observation (dict): The observation of the game after the action was taken.\n",
        "    action_taken (dict): The action taken by the agent.\n",
        "\n",
        "    Returns:\n",
        "    float: The calculated intrinsic reward.\n",
        "    \"\"\"\n",
        "    intrinsic_reward = 0.0\n",
        "    set_reward = {'reward_successful_play': 1 ,'penalty_wrong_play': -1  ,'discard_useless': 1,'discard_useful': -1}\n",
        "\n",
        "\n",
        "    cur_fireworks = current_observation['fireworks']\n",
        "\n",
        "    try:\n",
        "        action_idx = current_observation['legal_moves_as_int'].index(action_taken)\n",
        "        action_dict = current_observation['legal_moves'][action_idx]\n",
        "    except ValueError:\n",
        "        # Handle the case where action_id is not in legal_moves_as_int\n",
        "        return intrinsic_reward\n",
        "\n",
        "    action_type = action_dict['action_type']\n",
        "    cur_player_observed_hands = current_observation['observed_hands']\n",
        "    card_knowledge = current_observation['card_knowledge']\n",
        "\n",
        "\n",
        "    # Reward for certainty / progress in fireworks and successful play\n",
        "\n",
        "    if action_type == 'PLAY' or action_type =='DISCARD':\n",
        "\n",
        "    # Reward for card certainty:\n",
        "        certainty_score=0.1\n",
        "\n",
        "        card_index = action_dict['card_index']\n",
        "        if card_index >= len(card_knowledge[0]):\n",
        "            return False  # Invalid card index\n",
        "\n",
        "        card = card_knowledge[0][card_index]\n",
        "        if card['color'] is not None: certainty_score += 0.45\n",
        "        if card['rank'] is not None: certainty_score += 0.45\n",
        "\n",
        "        if action_type == 'PLAY':\n",
        "\n",
        "      #check successful or failed play\n",
        "          fireworks_progress = sum(next_observation['fireworks'].values()) - \\\n",
        "                              sum(current_observation['fireworks'].values())\n",
        "          if fireworks_progress > 0:\n",
        "              intrinsic_reward += certainty_score * set_reward['reward_successful_play']\n",
        "          else:\n",
        "              intrinsic_reward += certainty_score * set_reward['penalty_wrong_play']\n",
        "    # Reward for strategic discard\n",
        "        else:\n",
        "          #rank might be None so we get it from the discarded pile in next observation\n",
        "          known_discarded_card= next_observation['discard_pile'][len(next_observation['discard_pile'])-1]\n",
        "          color=known_discarded_card['color']\n",
        "          fireworks = current_observation['fireworks']\n",
        "          if known_discarded_card['rank'] == fireworks[color]+1:\n",
        "            intrinsic_reward+= certainty_score * set_reward['discard_useful']\n",
        "          else:\n",
        "            intrinsic_reward+= certainty_score * set_reward['discard_useless']\n",
        "\n",
        "    return intrinsic_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9TUiwmR4Ahk"
      },
      "source": [
        "##### Certain Agent with strategic discard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnQWVf8O4MZ6"
      },
      "outputs": [],
      "source": [
        "def calculate_certain_strategic_reward(current_observation,next_observation, action_taken):\n",
        "    \"\"\"\n",
        "    Calculate the intrinsic reward\n",
        "\n",
        "    Parameters:\n",
        "    previous_observation (dict): The observation of the game before the action was taken.\n",
        "    current_observation (dict): The observation of the game after the action was taken.\n",
        "    action_taken (dict): The action taken by the agent.\n",
        "\n",
        "    Returns:\n",
        "    float: The calculated intrinsic reward.\n",
        "    \"\"\"\n",
        "    intrinsic_reward = 0.0\n",
        "    set_reward = {'reward_successful_play': 1 ,'penalty_wrong_play': -1  ,'discard_useless': 1,'discard_useful': -1}\n",
        "\n",
        "\n",
        "    cur_fireworks = current_observation['fireworks']\n",
        "\n",
        "    try:\n",
        "        action_idx = current_observation['legal_moves_as_int'].index(action_taken)\n",
        "        action_dict = current_observation['legal_moves'][action_idx]\n",
        "    except ValueError:\n",
        "        # Handle the case where action_id is not in legal_moves_as_int\n",
        "        return intrinsic_reward\n",
        "\n",
        "    action_type = action_dict['action_type']\n",
        "    cur_player_observed_hands = current_observation['observed_hands']\n",
        "    card_knowledge = current_observation['card_knowledge']\n",
        "\n",
        "\n",
        "\n",
        "    # Reward for certainty / progress in fireworks and successful play\n",
        "\n",
        "    if action_type == 'PLAY':\n",
        "\n",
        "    # Reward for card certainty:\n",
        "        certainty_score=0.1\n",
        "        card_index = action_dict['card_index']\n",
        "\n",
        "        if card_index >= len(card_knowledge[0]):\n",
        "            return False  # Invalid card index\n",
        "\n",
        "        card = card_knowledge[0][card_index]\n",
        "        if card['color'] is not None: certainty_score += 0.45\n",
        "        if card['rank'] is not None: certainty_score += 0.45\n",
        "\n",
        "    #check successful or failed play\n",
        "        fireworks_progress = sum(next_observation['fireworks'].values()) - \\\n",
        "                             sum(current_observation['fireworks'].values())\n",
        "        if fireworks_progress > 0:\n",
        "            intrinsic_reward += certainty_score * set_reward['reward_successful_play']\n",
        "        else:\n",
        "            intrinsic_reward += certainty_score * set_reward['penalty_wrong_play']\n",
        "    # Reward for strategic discard\n",
        "\n",
        "    if action_type == 'DISCARD':\n",
        "\n",
        "      player_hand = current_observation['card_knowledge'][0]\n",
        "      card_index = action_dict['card_index']\n",
        "      discarded_card = player_hand[card_index]\n",
        "      known_discarded_card= next_observation['discard_pile'][len(next_observation['discard_pile'])-1]\n",
        "      color=discarded_card['color']\n",
        "      rank=discarded_card['rank']\n",
        "      fireworks = current_observation['fireworks']\n",
        "\n",
        "      certainty_score=0.1\n",
        "\n",
        "      if color is not None: certainty_score += 0.3\n",
        "      if rank is not None:  certainty_score += 0.3\n",
        "\n",
        "      #treating special cases where we know we can discad the card even without full information:\n",
        "\n",
        "      # completely useless card that is not needed anymore to finish fireworks\n",
        "      if rank is not None and color is not None and rank <= fireworks[color] :\n",
        "        certainty_score += 0.3\n",
        "\n",
        "      #if we only know the rank and it is inferior to the minimum card value on the table then it can be discarded for sure\n",
        "      elif rank is not None and color is None and rank <= min(fireworks.values()):\n",
        "            certainty_score=1\n",
        "      #final score\n",
        "      if known_discarded_card['rank'] == fireworks[known_discarded_card['color']]+1:\n",
        "            intrinsic_reward+= certainty_score * set_reward['discard_useful']\n",
        "      else:\n",
        "            intrinsic_reward+= certainty_score * set_reward['discard_useless']\n",
        "    # Penalty or reward for information token usage\n",
        "    info_token_difference = next_observation['information_tokens'] - \\\n",
        "                            current_observation['information_tokens']\n",
        "    intrinsic_reward += info_token_difference\n",
        "\n",
        "    return intrinsic_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtRjY3pb3yeU"
      },
      "source": [
        "#### Balanced Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYcgXyNk1F2r"
      },
      "outputs": [],
      "source": [
        "def calculate_balanced_reward(current_observation,next_observation, action_taken):\n",
        "    \"\"\"\n",
        "    Calculate the intrinsic reward\n",
        "\n",
        "    Parameters:\n",
        "    previous_observation (dict): The observation of the game before the action was taken.\n",
        "    current_observation (dict): The observation of the game after the action was taken.\n",
        "    action_taken (dict): The action taken by the agent.\n",
        "\n",
        "    Returns:\n",
        "    float: The calculated intrinsic reward.\n",
        "    \"\"\"\n",
        "    intrinsic_reward = 0.0\n",
        "    set_reward = {'reward_successful_play': 0.2 ,'penalty_wrong_play': -0.2  ,'discard_useless': 0.2,'discard_useful': -0.2,\n",
        "                  'reveal_play': 0.2,'repeat_reveal': -0.2}\n",
        "\n",
        "\n",
        "    cur_fireworks = current_observation['fireworks']\n",
        "\n",
        "    try:\n",
        "        action_idx = current_observation['legal_moves_as_int'].index(action_taken)\n",
        "        action_dict = current_observation['legal_moves'][action_idx]\n",
        "    except ValueError:\n",
        "        # Handle the case where action_id is not in legal_moves_as_int\n",
        "        return intrinsic_reward\n",
        "\n",
        "    action_type = action_dict['action_type']\n",
        "    cur_player_observed_hands = current_observation['observed_hands']\n",
        "    card_knowledge = current_observation['card_knowledge']\n",
        "\n",
        "    # Reward for giving hints (cooperation)\n",
        "\n",
        "    if 10 <= action_taken <= 19:\n",
        "            intrinsic_reward += set_reward['reveal_play']\n",
        "\n",
        "    if action_type in ['REVEAL_COLOR', 'REVEAL_RANK']:\n",
        "        target_offset = action_dict['target_offset']\n",
        "        target_hand = cur_player_observed_hands[target_offset]\n",
        "        target_knowledge = card_knowledge[target_offset]\n",
        "\n",
        "        if action_type == 'REVEAL_COLOR':\n",
        "            color = action_dict['color']\n",
        "            for card_idx, card in enumerate(target_hand):\n",
        "                if target_knowledge[card_idx]['color'] == color:\n",
        "                    intrinsic_reward += set_reward['repeat_reveal']\n",
        "                elif card['color'] == color and card['rank'] == cur_fireworks[color]:\n",
        "                    intrinsic_reward += set_reward['reveal_play']\n",
        "\n",
        "        else:  # REVEAL_RANK\n",
        "            rank = action_dict['rank']\n",
        "            for card_idx, card in enumerate(target_hand):\n",
        "                if target_knowledge[card_idx]['rank'] == rank:\n",
        "                    intrinsic_reward += set_reward['repeat_reveal']\n",
        "                elif card['rank'] == rank and cur_fireworks[card['color']] == rank:\n",
        "                    intrinsic_reward += set_reward['reveal_play']\n",
        "\n",
        "\n",
        "\n",
        "    # Reward for certainty / progress in fireworks and successful play\n",
        "\n",
        "    if action_type == 'PLAY':\n",
        "\n",
        "    # Reward for card certainty:\n",
        "        certainty_score=0.2\n",
        "\n",
        "        card_index = action_dict['card_index']\n",
        "\n",
        "        if card_index >= len(card_knowledge[0]):\n",
        "            return False  # Invalid card index\n",
        "\n",
        "        card = card_knowledge[0][card_index]\n",
        "        if card['color'] is not None: certainty_score += 0.4\n",
        "        if card['rank'] is not None: certainty_score += 0.4\n",
        "\n",
        "    #check successful or failed play\n",
        "        fireworks_progress = sum(next_observation['fireworks'].values()) - \\\n",
        "                             sum(current_observation['fireworks'].values())\n",
        "        if fireworks_progress > 0:\n",
        "            intrinsic_reward += set_reward['reward_successful_play']\n",
        "        else:\n",
        "            intrinsic_reward += set_reward['penalty_wrong_play']\n",
        "\n",
        "\n",
        "    # Reward for strategic discard\n",
        "\n",
        "    if action_type == 'DISCARD':\n",
        "      player_hand = current_observation['card_knowledge'][0]\n",
        "      card_index = action_dict['card_index']\n",
        "      discarded_card = player_hand[card_index]\n",
        "      known_discarded_card= next_observation['discard_pile'][len(next_observation['discard_pile'])-1]\n",
        "      color=discarded_card['color']\n",
        "      rank=discarded_card['rank']\n",
        "      fireworks = current_observation['fireworks']\n",
        "\n",
        "      certainty_score=0.1\n",
        "\n",
        "      if color is not None: certainty_score += 0.3\n",
        "      if rank is not None:  certainty_score += 0.3\n",
        "\n",
        "      #treating special cases where we know we can discad the card even without full information:\n",
        "\n",
        "      # completely useless card that is not needed anymore to finish fireworks\n",
        "      if rank is not None and color is not None and rank <= fireworks[color] :\n",
        "        certainty_score += 0.3\n",
        "\n",
        "      #if we only know the rank and it is inferior to the minimum card value on the table then it can be discarded for sure\n",
        "      elif rank is not None and color is None and rank <= min(fireworks.values()):\n",
        "            certainty_score=1\n",
        "      #final score\n",
        "      if known_discarded_card['rank'] == fireworks[known_discarded_card['color']]+1:\n",
        "            intrinsic_reward+= certainty_score * set_reward['discard_useful']\n",
        "      else:\n",
        "            intrinsic_reward+= certainty_score * set_reward['discard_useless']\n",
        "    # Penalty or reward for information token usage\n",
        "    info_token_difference = next_observation['information_tokens'] - \\\n",
        "                            current_observation['information_tokens']\n",
        "    intrinsic_reward += info_token_difference\n",
        "\n",
        "\n",
        "    return intrinsic_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRqjqKzjML_4"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhGnPPidn-s-"
      },
      "outputs": [],
      "source": [
        "def play_by_model(env, model: FCModel, agent_class, *, epsilon=0, games=100, record_out=None):\n",
        "  \"\"\"evaluate the playing performance of a given model with agent_class\"\"\"\n",
        "  N = env.N\n",
        "  episode_returns = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for e in range(games):\n",
        "    intrinsic_rewards = []\n",
        "    cooperation_rate=0\n",
        "    play_discard_rate=0\n",
        "    steps=0\n",
        "    env.reset()\n",
        "    agents = [agent_class(_, model) for _ in range(N)]\n",
        "    while not env.done:\n",
        "      steps+=1\n",
        "    # Get the current observation\n",
        "      current_observation = env.obs(env.to_play)\n",
        "      # choose an action\n",
        "      for agent_id, agent in enumerate(agents):\n",
        "        if env.to_play == agent_id:\n",
        "          action = agent.act(env, epsilon)\n",
        "        else:\n",
        "          agent.act(env)  # observation to be utilized future\n",
        "\n",
        "      #keep track of action nature of the agent\n",
        "      if 0 <= action <=9:\n",
        "        play_discard_rate+=1\n",
        "      else:\n",
        "        cooperation_rate+=1\n",
        "      env.step(action)\n",
        "      #save intrinsic reward\n",
        "      next_observation = env.obs(env.to_play)\n",
        "      env.intrinsic_reward = calculate_balanced_reward(current_observation,next_observation, action)\n",
        "      hanabi_score=sum(env.obs(env.to_play)['fireworks'].values())\n",
        "\n",
        "      #save episode return\n",
        "      intrinsic_rewards.append(env.intrinsic_reward)\n",
        "\n",
        "    episode_returns.append(sum(intrinsic_rewards))\n",
        "    #episode_returns.append(env.record.episode_return())\n",
        "\n",
        "    #save final score /cooperation rate and play/discard rate\n",
        "    env.record.hanabi_scores.append(hanabi_score)\n",
        "    env.record.cooperation_rate.append(cooperation_rate/steps)\n",
        "    env.record.play_discard_rate.append(play_discard_rate/steps)\n",
        "    if record_out is not None:\n",
        "      record_out.append(env.record)\n",
        "  return episode_returns,env.record.hanabi_scores,env.record.cooperation_rate,env.record.play_discard_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XDusc1g2X34"
      },
      "source": [
        "## DQN and variants\n",
        "\n",
        "see also pytorch tutorial https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzFdNvwK2tLw"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqzeM2tJmtZb"
      },
      "outputs": [],
      "source": [
        "def rows2cols(rows):\n",
        "  C = len(rows[0])\n",
        "  return [[r[c] for r in rows] for c in range(C)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HgsSjmKlmSQ"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "  \"\"\"buffer to keep past experiences as a sequence of transitions (s,a,r,s',done)\"\"\"\n",
        "  def __init__(self, limit: int=100_000):\n",
        "    self.buffer = []\n",
        "    self.limit = limit\n",
        "    self.cur = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.buffer) >= self.limit:\n",
        "      self.buffer[self.cur] = transition\n",
        "      self.cur = (self.cur+1) % self.limit\n",
        "    else:\n",
        "      self.buffer.append(transition)\n",
        "\n",
        "  def __getitem__(self, idx: int):\n",
        "    return self.buffer[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def sample(self, k: int=1):\n",
        "    rows = random.sample(self.buffer, k)\n",
        "    return rows2cols(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPpicVdtoMLU"
      },
      "outputs": [],
      "source": [
        "def train_minibatch(model, optimizer, criterion, target_model, samples, gamma):\n",
        "  optimizer.zero_grad()\n",
        "  states, actions, rewards, succ_states, has_successors = samples\n",
        "  q_values = model(torch.Tensor(np.array(states)))\n",
        "  action_index = torch.Tensor(actions).to(torch.int64).unsqueeze(-1)\n",
        "  # print(f'{q_values.shape=}, {action_index.shape=}')\n",
        "  q_values = q_values.gather(1, action_index.to(device))\n",
        "  target_qs = torch.Tensor(rewards).to(device)\n",
        "  successor_mask = torch.tensor(has_successors, dtype=torch.bool)\n",
        "  with torch.no_grad():\n",
        "    target_q_all = target_model(torch.Tensor(np.array(succ_states)))\n",
        "    # print(f'{target_q_all.shape=}, {successor_mask.shape=}, {x.shape=}')\n",
        "    # print(f'{target_qs[successor_mask].shape}')\n",
        "    target_qs[successor_mask] += gamma * target_q_all[successor_mask].max(1).values\n",
        "\n",
        "  loss = criterion(q_values, target_qs.unsqueeze(-1))\n",
        "  loss.backward()\n",
        "  torch.nn.utils.clip_grad_value_(model.parameters(), 10)\n",
        "  optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2G6lG3L9MQT"
      },
      "outputs": [],
      "source": [
        "def train_agents(env, buffer, model, optimizer, target_model, step_limit,\n",
        "                 agent_class,\n",
        "                 eps_threshold=0.5, batch_size=1, gamma=0.99):\n",
        "  criterion = torch.nn.MSELoss()\n",
        "  N = env.N\n",
        "  episode_returns = []\n",
        "  hanabi_scores=[]\n",
        "  intrinsic_rewards=[]\n",
        "  steps=0\n",
        "  cooperation_rate=0\n",
        "  play_discard_rate=0\n",
        "  agents = [agent_class(_, model) for _ in range(N)]\n",
        "  env.reset()\n",
        "\n",
        "  target_model.eval()           # no training for this model\n",
        "  model.train()\n",
        "\n",
        "  for t in range(step_limit):\n",
        "    steps+=1\n",
        "    # Get the current observation\n",
        "    current_observation = env.obs(env.to_play)\n",
        "\n",
        "\n",
        "    # choose an action\n",
        "    for agent_id, agent in enumerate(agents):\n",
        "      if env.to_play == agent_id:\n",
        "        action = agent.act(env, eps_threshold)\n",
        "\n",
        "      else:\n",
        "        agent.act(env)  # observation to be utilized future\n",
        "      #keep track of action nature of the agent\n",
        "    if 0 <= action <=9:\n",
        "        play_discard_rate+=1\n",
        "    else:\n",
        "        cooperation_rate+=1\n",
        "\n",
        "    prev_local_state = agents[env.to_play].make_observation(env)\n",
        "\n",
        "    env.step(action)\n",
        "    # Get the new observation\n",
        "    next_observation = env.obs(env.to_play)\n",
        "     # Calculate intrinsic reward\n",
        "    env.intrinsic_reward = calculate_balanced_reward(current_observation,next_observation, action)\n",
        "\n",
        "    #save intrinsic rewards\n",
        "    intrinsic_rewards.append(env.intrinsic_reward)\n",
        "    \"\"\"\n",
        "    total_reward = env.reward + env.intrinsic_reward\n",
        "    transition = (prev_local_state, action, total_reward,\n",
        "                  agents[env.to_play].make_observation(env),\n",
        "                  not env.done)\n",
        "    \"\"\"\n",
        "    transition = (prev_local_state, action, env.intrinsic_reward,\n",
        "                  agents[env.to_play].make_observation(env), not env.done)\n",
        "    buffer.add(transition)\n",
        "\n",
        "    if env.done:\n",
        "\n",
        "      #score of the game at the end\n",
        "      hanabi_score=sum(env.obs(env.to_play)['fireworks'].values())\n",
        "      hanabi_scores.append(hanabi_score)\n",
        "\n",
        "      #print(cooperation_rate/steps,play_discard_rate/steps)\n",
        "\n",
        "\n",
        "    # prepare new game\n",
        "\n",
        "      #episode_returns.append(env.record.episode_return())\n",
        "      episode_returns.append(sum(intrinsic_rewards))\n",
        "      intrinsic_rewards=[]\n",
        "      env.reset()\n",
        "      agents = [agent_class(_, model) for _ in range(N)]\n",
        "      steps=0\n",
        "      cooperation_rate=0\n",
        "      play_discard_rate=0\n",
        "\n",
        "    # learning\n",
        "    if len(buffer) >= batch_size and t % batch_size == 0:\n",
        "      train_minibatch(model, optimizer, criterion, target_model,\n",
        "                      buffer.sample(batch_size), gamma)\n",
        "\n",
        "  return episode_returns,hanabi_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw6_UU9BoSSw"
      },
      "outputs": [],
      "source": [
        "def train_main(env, model, target_model, optimizer, agent_class,\n",
        "               *, buffer=None, interval=8000, batch_size=64,\n",
        "               eps_start=0.9, eps_end=0.1, eps_decay=10,\n",
        "               repeat=200, eval_games=2, tau = 0.25):\n",
        "  if buffer is None:\n",
        "    buffer = ReplayBuffer()\n",
        "  training_returns = []\n",
        "  evaluation_returns = []\n",
        "  epsilons = []\n",
        "  hanabi_scores=[]\n",
        "  cooperation_scores=[]\n",
        "  play_discard_scores=[]\n",
        "\n",
        "  for tt in range(repeat):\n",
        "    # eps = eps_init * (eps_decay ** tt)\n",
        "    eps = eps_end + (eps_start - eps_end) * math.exp(-1. * tt / eps_decay)\n",
        "\n",
        "    epsilons.append(eps)\n",
        "    returns,train_scores= train_agents(env, buffer, model, optimizer, target_model, interval,\n",
        "                           eps_threshold=eps, batch_size=batch_size,\n",
        "                           agent_class=agent_class)\n",
        "    #average training rewards:\n",
        "    mean = sum(returns) / len(returns)\n",
        "    training_returns.append(mean)\n",
        "\n",
        "    #best Hanabi score in training\n",
        "    train_best_score=max(train_scores)\n",
        "\n",
        "    # soft update of target_model\n",
        "    parameters = model.state_dict()\n",
        "    target_parameters = target_model.state_dict()\n",
        "    for key in parameters:\n",
        "      target_parameters[key] = parameters[key]*tau + target_parameters[key]*(1-tau)\n",
        "    target_model.load_state_dict(target_parameters)\n",
        "\n",
        "    # evaluation\n",
        "    ereturns,eval_scores,cooperation_rates,play_discard_rates= play_by_model(env, model, games=eval_games, agent_class=agent_class)#try increase eval_games outside of training loop\n",
        "    print(cooperation_rates,play_discard_rates)\n",
        "    #average evaluation rewards:\n",
        "    eval_mean = sum(ereturns) / len(ereturns)\n",
        "    evaluation_returns.append(eval_mean)\n",
        "\n",
        "    #best Hanabi score in evaluation\n",
        "    eval_best_score=max(eval_scores)\n",
        "    hanabi_scores.append(eval_best_score)\n",
        "\n",
        "    #cooperation/non-cooperation action-ratios:\n",
        "    cooperation_mean=sum(cooperation_rates) / len(cooperation_rates)\n",
        "    play_discard_mean=sum(play_discard_rates) / len(play_discard_rates)\n",
        "    cooperation_scores.append(cooperation_mean)\n",
        "    play_discard_scores.append(play_discard_mean)\n",
        "\n",
        "\n",
        "    logging.info(f'{tt=} {mean=:5.2f} {eps=:5.2f} {eval_mean=:5.2f} {train_best_score=:5.2f} {eval_best_score=:5.2f}')\n",
        "  return training_returns, evaluation_returns, epsilons, hanabi_scores,cooperation_scores,play_discard_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB_zhA961avY"
      },
      "outputs": [],
      "source": [
        "    # evaluation\n",
        "def eval(env,model,agent_class,eval_games=2):\n",
        "      evaluation_returns = []\n",
        "      ereturns,eval_scores,cooperation_rates,play_discard_rates= play_by_model(env, model, games=eval_games, agent_class=agent_class)#try increase eval_games outside of training loop\n",
        "      print(cooperation_rates,play_discard_rates)\n",
        "      print(ereturns)\n",
        "      #average evaluation rewards:\n",
        "      eval_mean = sum(ereturns) / len(ereturns)\n",
        "      evaluation_returns.append(eval_mean)\n",
        "\n",
        "      #best Hanabi score in evaluation\n",
        "      eval_best_score=max(eval_scores)\n",
        "\n",
        "\n",
        "      #cooperation/non-cooperation action-ratios:\n",
        "      cooperation_mean=sum(cooperation_rates) / len(cooperation_rates)\n",
        "      play_discard_mean=sum(play_discard_rates) / len(play_discard_rates)\n",
        "\n",
        "      logging.info(f'{eval_mean=:5.2f} {eval_best_score=:5.2f} {cooperation_mean=:5.2f} {play_discard_mean=:5.2f}')\n",
        "      return evaluation_returns, hanabi_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q93DDGKnoYUk"
      },
      "source": [
        "### fair agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPgoGVMloXyX"
      },
      "outputs": [],
      "source": [
        "class FairAgent:\n",
        "  def __init__(self, agent_id: int, model):\n",
        "    self.model = model\n",
        "    self.agent_id = agent_id\n",
        "\n",
        "  def make_observation(self, env):\n",
        "    return np.array(env.vector_obs())\n",
        "\n",
        "  def act(self, env, epsilon=0) -> int:\n",
        "    if env.to_play != self.agent_id:\n",
        "      return None\n",
        "    legal_moves = env.legal_moves()\n",
        "    p = random.random()\n",
        "    if p < epsilon:\n",
        "      return random.choice(legal_moves)\n",
        "    with torch.no_grad():\n",
        "      obs = self.make_observation(env)\n",
        "      batch = obs[np.newaxis, :]       # make a single-element batch\n",
        "      qs = self.model(torch.from_numpy(batch)).detach().to('cpu')\n",
        "      qs = qs[0]  # retrieve the first element in the batch\n",
        "      qs -= qs.min()\n",
        "      # zero-out illegal moves\n",
        "      qs *= index_to_binaryvector(legal_moves, self.model.action_dim)\n",
        "    return qs.argmax().item() #rerurns int action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJMa31yVogsu"
      },
      "source": [
        "#### experiments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "#device = 'cpu'\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEeho_CestV8",
        "outputId": "45122c44-5ec4-485e-d33f-faf652f8d12b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj_i8-SRofM4",
        "outputId": "2d6ba491-cf85-49f1-be61-7b4e66e33eeb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:09:23,980 INFO model obs_dim=658, action_dim=20, device='cuda'\n",
            "2024-01-04 06:09:23,987 INFO model obs_dim=658, action_dim=20, device='cuda'\n",
            "2024-01-04 06:10:50,758 INFO tt=0 mean=-0.86 eps= 0.90 eval_mean=-0.40 train_best_score= 9.00 eval_best_score= 0.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0] [1.0]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:12:16,511 INFO tt=1 mean=-0.71 eps= 0.88 eval_mean=-0.60 train_best_score=11.00 eval_best_score= 0.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0] [1.0]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:13:45,887 INFO tt=2 mean=-0.65 eps= 0.87 eval_mean=-0.60 train_best_score= 9.00 eval_best_score= 0.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0] [1.0]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:15:14,906 INFO tt=3 mean=-0.53 eps= 0.85 eval_mean=-0.60 train_best_score= 9.00 eval_best_score= 0.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0] [1.0]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:16:45,358 INFO tt=4 mean=-0.45 eps= 0.84 eval_mean=-0.50 train_best_score=10.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0] [1.0]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:18:16,484 INFO tt=5 mean=-0.40 eps= 0.82 eval_mean= 0.50 train_best_score= 9.00 eval_best_score= 0.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.2] [0.8]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:19:47,524 INFO tt=6 mean=-0.30 eps= 0.81 eval_mean= 0.10 train_best_score=10.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0] [1.0]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:21:20,107 INFO tt=7 mean=-0.23 eps= 0.80 eval_mean= 0.02 train_best_score= 9.00 eval_best_score= 0.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.2] [0.8]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:22:54,636 INFO tt=8 mean=-0.20 eps= 0.78 eval_mean= 9.78 train_best_score= 8.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:24:28,419 INFO tt=9 mean=-0.13 eps= 0.77 eval_mean=12.45 train_best_score= 9.00 eval_best_score= 5.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4533333333333333] [0.5466666666666666]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:26:02,709 INFO tt=10 mean=-0.05 eps= 0.75 eval_mean=11.95 train_best_score=11.00 eval_best_score= 0.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4875] [0.5125]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:27:38,590 INFO tt=11 mean=-0.01 eps= 0.74 eval_mean=13.27 train_best_score= 7.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:29:14,164 INFO tt=12 mean= 0.06 eps= 0.73 eval_mean=12.67 train_best_score= 9.00 eval_best_score= 5.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4533333333333333] [0.5466666666666666]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:30:51,564 INFO tt=13 mean= 0.08 eps= 0.72 eval_mean=10.94 train_best_score= 9.00 eval_best_score= 0.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4875] [0.5125]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:32:28,327 INFO tt=14 mean= 0.10 eps= 0.70 eval_mean=13.17 train_best_score= 8.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:34:06,820 INFO tt=15 mean= 0.22 eps= 0.69 eval_mean=11.83 train_best_score=11.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:35:45,210 INFO tt=16 mean= 0.24 eps= 0.68 eval_mean=10.32 train_best_score= 9.00 eval_best_score= 4.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4605263157894737] [0.5394736842105263]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:37:23,589 INFO tt=17 mean= 0.30 eps= 0.67 eval_mean=10.91 train_best_score=13.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:39:03,291 INFO tt=18 mean= 0.34 eps= 0.66 eval_mean=12.23 train_best_score= 8.00 eval_best_score= 0.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4875] [0.5125]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:40:48,005 INFO tt=19 mean= 0.39 eps= 0.65 eval_mean= 9.12 train_best_score= 9.00 eval_best_score= 3.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4675324675324675] [0.5324675324675324]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:42:30,103 INFO tt=20 mean= 0.44 eps= 0.64 eval_mean=12.15 train_best_score= 8.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:44:11,184 INFO tt=21 mean= 0.46 eps= 0.63 eval_mean=11.88 train_best_score= 8.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:45:53,387 INFO tt=22 mean= 0.50 eps= 0.62 eval_mean= 9.15 train_best_score= 9.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:47:36,282 INFO tt=23 mean= 0.53 eps= 0.61 eval_mean=10.99 train_best_score=11.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:49:18,995 INFO tt=24 mean= 0.57 eps= 0.60 eval_mean= 6.64 train_best_score= 9.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:51:02,603 INFO tt=25 mean= 0.61 eps= 0.59 eval_mean=10.85 train_best_score= 9.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4696969696969697] [0.5303030303030303]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:52:46,232 INFO tt=26 mean= 0.63 eps= 0.58 eval_mean=11.44 train_best_score= 9.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:54:30,843 INFO tt=27 mean= 0.67 eps= 0.57 eval_mean= 9.00 train_best_score=10.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:56:15,742 INFO tt=28 mean= 0.70 eps= 0.56 eval_mean=10.40 train_best_score=10.00 eval_best_score= 3.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4675324675324675] [0.5324675324675324]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:58:01,119 INFO tt=29 mean= 0.72 eps= 0.55 eval_mean=12.56 train_best_score= 9.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 06:59:47,959 INFO tt=30 mean= 0.77 eps= 0.54 eval_mean= 9.80 train_best_score= 9.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 07:01:33,465 INFO tt=31 mean= 0.75 eps= 0.53 eval_mean=10.99 train_best_score= 9.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 07:03:20,029 INFO tt=32 mean= 0.81 eps= 0.52 eval_mean=11.55 train_best_score= 9.00 eval_best_score= 3.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4675324675324675] [0.5324675324675324]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 07:05:08,370 INFO tt=33 mean= 0.84 eps= 0.51 eval_mean= 9.91 train_best_score=10.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 07:06:57,050 INFO tt=34 mean= 0.85 eps= 0.51 eval_mean=13.45 train_best_score=10.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 07:08:45,056 INFO tt=35 mean= 0.89 eps= 0.50 eval_mean= 9.17 train_best_score= 9.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 07:10:34,200 INFO tt=36 mean= 0.91 eps= 0.49 eval_mean= 8.56 train_best_score=11.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 07:12:23,128 INFO tt=37 mean= 0.94 eps= 0.48 eval_mean=11.23 train_best_score=10.00 eval_best_score= 3.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4675324675324675] [0.5324675324675324]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 07:14:13,562 INFO tt=38 mean= 0.94 eps= 0.47 eval_mean= 7.49 train_best_score= 9.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4875] [0.5125]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 07:16:03,242 INFO tt=39 mean= 0.99 eps= 0.47 eval_mean= 8.15 train_best_score= 9.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:17:53,026 INFO tt=40 mean= 0.96 eps= 0.46 eval_mean= 8.35 train_best_score= 9.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:19:39,443 INFO tt=41 mean= 1.03 eps= 0.45 eval_mean= 4.87 train_best_score= 8.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.2857142857142857] [0.7142857142857143]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:21:26,665 INFO tt=42 mean= 1.04 eps= 0.45 eval_mean=10.01 train_best_score= 8.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:23:14,621 INFO tt=43 mean= 1.04 eps= 0.44 eval_mean=10.62 train_best_score= 9.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:25:02,377 INFO tt=44 mean= 1.02 eps= 0.43 eval_mean=10.47 train_best_score= 8.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:26:51,081 INFO tt=45 mean= 1.01 eps= 0.43 eval_mean= 8.29 train_best_score= 9.00 eval_best_score= 3.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4675324675324675] [0.5324675324675324]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:28:39,520 INFO tt=46 mean= 1.02 eps= 0.42 eval_mean= 8.13 train_best_score=10.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:30:28,505 INFO tt=47 mean= 1.05 eps= 0.41 eval_mean= 6.78 train_best_score= 8.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:32:17,747 INFO tt=48 mean= 1.05 eps= 0.41 eval_mean= 6.99 train_best_score= 9.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:34:07,032 INFO tt=49 mean= 1.05 eps= 0.40 eval_mean= 7.02 train_best_score= 8.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:35:57,786 INFO tt=50 mean= 1.06 eps= 0.39 eval_mean= 5.01 train_best_score=10.00 eval_best_score= 3.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4675324675324675] [0.5324675324675324]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:37:47,912 INFO tt=51 mean= 1.08 eps= 0.39 eval_mean= 5.17 train_best_score=11.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:39:37,611 INFO tt=52 mean= 1.15 eps= 0.38 eval_mean= 4.72 train_best_score= 8.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:41:31,477 INFO tt=53 mean= 1.13 eps= 0.38 eval_mean= 2.58 train_best_score=12.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:43:30,850 INFO tt=54 mean= 1.15 eps= 0.37 eval_mean= 7.09 train_best_score= 9.00 eval_best_score= 5.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4605263157894737] [0.5394736842105263]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:45:28,078 INFO tt=55 mean= 1.22 eps= 0.37 eval_mean= 5.94 train_best_score= 8.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:47:28,307 INFO tt=56 mean= 1.16 eps= 0.36 eval_mean= 4.17 train_best_score= 9.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:49:22,293 INFO tt=57 mean= 1.14 eps= 0.36 eval_mean=-0.69 train_best_score= 9.00 eval_best_score= 4.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4605263157894737] [0.5394736842105263]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:51:13,867 INFO tt=58 mean= 1.20 eps= 0.35 eval_mean= 3.89 train_best_score= 8.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:53:06,633 INFO tt=59 mean= 1.23 eps= 0.35 eval_mean= 5.63 train_best_score= 9.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:54:59,463 INFO tt=60 mean= 1.21 eps= 0.34 eval_mean= 2.59 train_best_score= 8.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:56:52,712 INFO tt=61 mean= 1.24 eps= 0.34 eval_mean= 5.65 train_best_score= 8.00 eval_best_score= 3.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 07:58:46,302 INFO tt=62 mean= 1.25 eps= 0.33 eval_mean= 7.40 train_best_score=10.00 eval_best_score= 3.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4675324675324675] [0.5324675324675324]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 08:00:38,872 INFO tt=63 mean= 1.28 eps= 0.33 eval_mean= 5.09 train_best_score= 8.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 08:02:33,147 INFO tt=64 mean= 1.26 eps= 0.32 eval_mean= 4.67 train_best_score= 9.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4875] [0.5125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 08:04:26,821 INFO tt=65 mean= 1.33 eps= 0.32 eval_mean= 5.71 train_best_score= 9.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 08:06:21,196 INFO tt=66 mean= 1.30 eps= 0.31 eval_mean= 5.37 train_best_score= 9.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4810126582278481] [0.5189873417721519]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 08:08:16,247 INFO tt=67 mean= 1.33 eps= 0.31 eval_mean= 7.14 train_best_score=10.00 eval_best_score= 3.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4675324675324675] [0.5324675324675324]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 08:10:10,163 INFO tt=68 mean= 1.32 eps= 0.31 eval_mean= 5.40 train_best_score=10.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 08:12:05,077 INFO tt=69 mean= 1.32 eps= 0.30 eval_mean= 6.07 train_best_score=10.00 eval_best_score= 2.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47435897435897434] [0.5256410256410257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 08:14:00,335 INFO tt=70 mean= 1.32 eps= 0.30 eval_mean= 4.94 train_best_score= 9.00 eval_best_score= 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4722222222222222] [0.5277777777777778]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-04 08:15:56,851 INFO tt=71 mean= 1.46 eps= 0.29 eval_mean= 3.80 train_best_score=11.00 eval_best_score= 3.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.46835443037974683] [0.5316455696202531]\n"
          ]
        }
      ],
      "source": [
        "env = HanabiEnv()\n",
        "individual_obs_dim = len(env.vector_obs())\n",
        "action_dim = 20  # ?\n",
        "model = FCModel(individual_obs_dim, action_dim, device).to(device)\n",
        "target_model = FCModel(individual_obs_dim, action_dim, device).to(device)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "\n",
        "interval = 100000\n",
        "buffer = ReplayBuffer()\n",
        "train_r, eval_r, eps,hanabi_scores,cooperation_scores,play_discard_scores = train_main(env, model, target_model, optimizer,\n",
        "                                  FairAgent,\n",
        "                                  buffer=buffer,\n",
        "                                  interval=interval, repeat=300, eps_decay=50)\n",
        "eval_results, hanabi_scores= eval(env,model,FairAgent,eval_games=200)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GK1qYFyrWhcr"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_r, label='training')\n",
        "plt.plot(eval_r, label='evaluation (greedy)')\n",
        "plt.plot(eps, label='epsilon')\n",
        "plt.legend()\n",
        "plt.xlabel(f'learning step (x{interval})')\n",
        "plt.ylabel(f'episode return')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB3K5IrXuHeo"
      },
      "outputs": [],
      "source": [
        "#plt.plot(hanabi_scores, label='game score')\n",
        "plt.plot(cooperation_scores, label='cooperation rate')\n",
        "plt.plot(play_discard_scores, label='p/d rate')\n",
        "plt.legend()\n",
        "plt.xlabel(f'learning step (x{interval})')\n",
        "plt.ylabel(f'episode return')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb4q3qRDuS0n"
      },
      "outputs": [],
      "source": [
        "filename = 'fairmodeel.pth'\n",
        "torch.save(model.state_dict(), filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57bL1kQiuN1_"
      },
      "source": [
        "### cheating agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8YzOnr5uNU-"
      },
      "outputs": [],
      "source": [
        "class CheatingAgent:\n",
        "  def __init__(self, agent_id: int, model):\n",
        "    self.model = model\n",
        "    self.agent_id = agent_id\n",
        "\n",
        "  def make_observation(self, env):\n",
        "    all_obs = [env.vector_obs((env.to_play + i) % env.N) for i in range(env.N)]\n",
        "    return np.array(sum(all_obs, []))\n",
        "\n",
        "  def act(self, env, epsilon=0) -> int:\n",
        "    if env.to_play != self.agent_id:\n",
        "      return None\n",
        "    legal_moves = env.legal_moves()\n",
        "    p = random.random()\n",
        "    if p < epsilon:\n",
        "      return random.choice(legal_moves)\n",
        "    with torch.no_grad():\n",
        "      obs = self.make_observation(env)\n",
        "      batch = obs[np.newaxis, :]       # make a single-element batch\n",
        "      qs = self.model(torch.from_numpy(batch)).detach().to('cpu')\n",
        "      qs = qs[0]  # retrieve the first element in the batch\n",
        "      qs -= qs.min()\n",
        "      # zero-out illegal moves\n",
        "      qs *= index_to_binaryvector(legal_moves, self.model.action_dim)\n",
        "    return qs.argmax().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH7etNPyugFi"
      },
      "outputs": [],
      "source": [
        "env = HanabiEnv()\n",
        "indivisual_obs_dim = len(env.vector_obs())\n",
        "action_dim = 20  # ?\n",
        "cheat_model = FCModel(indivisual_obs_dim*2, action_dim, device).to(device)\n",
        "cheat_target_model = FCModel(indivisual_obs_dim*2, action_dim, device).to(device)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = torch.optim.AdamW(cheat_model.parameters())\n",
        "\n",
        "buffer = ReplayBuffer()\n",
        "interval = 2000\n",
        "\n",
        "ctrain_r, ceval_r, ceps = train_main(\n",
        "  env, cheat_model, cheat_target_model, optimizer,\n",
        "  buffer=buffer, interval=interval, repeat=200, eps_decay=50,\n",
        "  agent_class=CheatingAgent\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjb8RScLwnoY"
      },
      "outputs": [],
      "source": [
        "plt.plot(ctrain_r, label='training')\n",
        "plt.plot(ceval_r, label='evaluation (greedy)')\n",
        "plt.plot(ceps, label='epsilon')\n",
        "plt.legend()\n",
        "plt.xlabel(f'learning step (x{interval})')\n",
        "plt.ylabel(f'episode return')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-BP36ydw5_I"
      },
      "outputs": [],
      "source": [
        "cfilename = 'cheatingmodeel.pth'\n",
        "torch.save(cheat_model.state_dict(), cfilename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIB4hNNTwwDr"
      },
      "outputs": [],
      "source": [
        "record_store = []\n",
        "play_by_model(env, cheat_model, agent_class=CheatingAgent, games=10, record_out=record_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_708wxOUQOyS"
      },
      "source": [
        "### 3-player games"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55ymgzeTQUEC"
      },
      "source": [
        "#### fair agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnhXqfASQSDm"
      },
      "outputs": [],
      "source": [
        "env3 = HanabiEnv(num_players=3)\n",
        "indivisual_obs_dim3 = len(env3.vector_obs())\n",
        "action_dim = 20  # ?\n",
        "model3 = FCModel(indivisual_obs_dim3, action_dim, device).to(device)\n",
        "target_model3 = FCModel(indivisual_obs_dim3, action_dim, device).to(device)\n",
        "optimizer3 = torch.optim.AdamW(model3.parameters())\n",
        "\n",
        "interval = 2000\n",
        "buffer = ReplayBuffer()\n",
        "train_r3, eval_r3, eps3 = train_main(env3, model3, target_model3, optimizer3,\n",
        "                                     FairAgent,\n",
        "                                     buffer=buffer,\n",
        "                                     interval=interval, repeat=200, eps_decay=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlEqPAtCT713"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_r3, label='training')\n",
        "plt.plot(eval_r3, label='evaluation (greedy)')\n",
        "plt.plot(eps3, label='epsilon')\n",
        "plt.legend()\n",
        "plt.xlabel(f'learning step (x{interval})')\n",
        "plt.ylabel(f'episode return')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCRfOgriUP30"
      },
      "source": [
        "#### cheating agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oozmaDCoURNr"
      },
      "outputs": [],
      "source": [
        "env3 = HanabiEnv(num_players=3)\n",
        "indivisual_obs_dim3 = len(env3.vector_obs())\n",
        "action_dim = 30  # ?\n",
        "cheat_model3 = FCModel(indivisual_obs_dim3*3, action_dim, device).to(device)\n",
        "cheat_target_model3 = FCModel(indivisual_obs_dim3*3, action_dim, device).to(device)\n",
        "optimizer3c = torch.optim.AdamW(cheat_model3.parameters())\n",
        "\n",
        "buffer = ReplayBuffer()\n",
        "interval = 2000\n",
        "\n",
        "ctrain_r3, ceval_r3, c3eps = train_main(\n",
        "  env3, cheat_model3, cheat_target_model3, optimizer3c,\n",
        "  buffer=buffer, interval=interval, repeat=200, eps_decay=50,\n",
        "  agent_class=CheatingAgent\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed9cJdO7ZE1q"
      },
      "outputs": [],
      "source": [
        "plt.plot(ctrain_r3, label='training')\n",
        "plt.plot(ceval_r3, label='evaluation (greedy)')\n",
        "plt.plot(c3eps, label='epsilon')\n",
        "plt.legend()\n",
        "plt.xlabel(f'learning step (x{interval})')\n",
        "plt.ylabel(f'episode return')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
